from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
import time

# ---------------------
# âœ… åƒæ•¸è¨­å®š
# ---------------------
root_url = "https://oa-22.adm.ncu.edu.tw/"
target_file_url = "https://oa-22.adm.ncu.edu.tw/word/%E5%9C%8B%E7%AB%8B%E5%A4%A7%E5%AD%B8%E6%A0%A1%E9%99%A2%E6%A0%A1%E5%8B%99%E5%9F%BA%E9%87%91%E8%A8%AD%E7%BD%AE%E6%A2%9D%E4%BE%8B.pdf"
max_depth = 10
chromedriver_path = "chromedriver.exe"  # ä¿®æ”¹æˆä½ çš„ chromedriver è·¯å¾‘
max_workers = 10  # åŒæ™‚æœ€å¤šå¤šå°‘å€‹åŸ·è¡Œç·’

# ---------------------
# âœ… æ¯å±¤éè¿´çˆ¬é é¢é€£çµ
# ---------------------
def collect_all_links(driver, base_url, max_depth=2, current_depth=0, visited=None):
    if visited is None:
        visited = set()
    if current_depth > max_depth or base_url in visited:
        return visited

    visited.add(base_url)

    try:
        driver.get(base_url)
        time.sleep(1)
    except Exception as e:
        print(f"âŒ Failed to open {base_url}, error: {e}")
        return visited

    # ä¸€æ¬¡æ€§ç›´æ¥æŠ“ hrefsï¼Œé¦¬ä¸Šå­˜èµ·ä¾†
    hrefs = []
    try:
        elements = driver.find_elements(By.TAG_NAME, "a")
        for elem in elements:
            href = elem.get_attribute("href")
            if href:
                hrefs.append(href)
    except Exception as e:
        print(f"âš ï¸ Error when extracting hrefs from {base_url}: {e}")
        return visited

    # éè¿´è™•ç†æ–°æ‰¾åˆ°çš„ href
    for href in hrefs:
        if href.startswith(root_url) and href not in visited:
            visited = collect_all_links(driver, href, max_depth, current_depth + 1, visited)

    return visited


# ---------------------
# âœ… å¤šåŸ·è¡Œç·’æª¢æŸ¥æ¯å€‹é é¢æ˜¯å¦åŒ…å«ç›®æ¨™é€£çµ
# ---------------------
def check_pages(urls, target_file_url):
    # æ¯å€‹ thread è‡ªå·±ç¨ç«‹ driver
    options = Options()
    # options.add_argument("--headless")
    driver = webdriver.Chrome(service=Service(chromedriver_path), options=options)

    found_pages = []

    for page_url in urls:
        try:
            driver.get(page_url)
            time.sleep(1)  # ç­‰ JS æ¸²æŸ“

            html = driver.page_source
            if target_file_url in html:
                print(f"âœ… Found target on: {page_url}")
                found_pages.append(page_url)
        except Exception as e:
            print(f"âŒ Error on {page_url}: {e}")
            continue

    driver.quit()
    return found_pages

# ---------------------
# âœ… åˆ†æ‰¹åŸ·è¡Œå¤šåŸ·è¡Œç·’
# ---------------------
def run_multithread_check(all_page_urls, target_file_url, max_workers=3):
    results = []

    # æ¯å€‹ batch åˆ† 10 å€‹ URL
    batches = [all_page_urls[i:i + 10] for i in range(0, len(all_page_urls), 10)]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(check_pages, batch, target_file_url) for batch in batches]
        for f in futures:
            results.extend(f.result())

    return results

# ---------------------
# âœ… ä¸»æµç¨‹
# ---------------------
if __name__ == "__main__":
    # åˆå§‹åŒ– driver å…ˆæŠ“é€£çµ
    options = Options()
    # options.add_argument("--headless")
    driver = webdriver.Chrome(service=Service(chromedriver_path), options=options)

    print("ğŸ” Collecting all internal page URLs...")
    all_page_urls = collect_all_links(driver, root_url, max_depth=max_depth)
    driver.quit()
    all_page_urls = list(all_page_urls)
    print(f"âœ… Collected {len(all_page_urls)} pages.")

    # å¤šåŸ·è¡Œç·’æª¢æŸ¥
    found_pages = run_multithread_check(all_page_urls, target_file_url, max_workers=max_workers)

    print("\n--- âœ… Pages containing the target file link ---")
    for page in found_pages:
        print(page)
